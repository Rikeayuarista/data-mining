{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Biodata \u00b6 Nama : Rike Ayu Arista NIM : 180411100088 Matakuliah : Penambangan Data 5-D Program Studi : Teknik Informatika Fakultas : Teknik Perguruan Tinggi : Universitas Trunojoyo Madura","title":"index"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#biodata","text":"Nama : Rike Ayu Arista NIM : 180411100088 Matakuliah : Penambangan Data 5-D Program Studi : Teknik Informatika Fakultas : Teknik Perguruan Tinggi : Universitas Trunojoyo Madura","title":"Biodata"},{"location":"DECISION TREE/","text":"DECISION TREE \u00b6 decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Decision Tree"},{"location":"DECISION TREE/#decision-tree","text":"decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"DECISION TREE"},{"location":"DECISION TREE/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"CARA MEMBUAT DECISION TREE"},{"location":"Tugas 3/","text":"Missing Values using KNN \u00b6 KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan. Kalibrasi Parameter KNN \u00b6 Jumlah tetangga yang harus dicari \u00b6 Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan. Metode agregasi untuk digunakan \u00b6 Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal Normalisasi data \u00b6 Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan. Atribut numerik jarak \u00b6 Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...). Atribut kategorikal jarak \u00b6 tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0","title":"missing values"},{"location":"Tugas 3/#missing-values-using-knn","text":"KNN adalah algoritma yang berguna untuk mencocokkan suatu titik dengan tetangga terdekatnya dalam ruang multi-dimensi. Ini dapat digunakan untuk data yang kontinu, diskrit, ordinal, dan kategoris yang membuatnya sangat berguna untuk menangani semua jenis data yang hilang. Asumsi di balik menggunakan KNN untuk nilai yang hilang adalah bahwa nilai poin dapat didekati dengan nilai dari poin yang paling dekat dengannya, berdasarkan pada variabel lain. Mari kita simpan contoh sebelumnya dan tambahkan variabel lain, penghasilan orang tersebut. Sekarang kami memiliki tiga variabel, jenis kelamin, pendapatan dan tingkat depresi yang memiliki nilai yang hilang. Kami kemudian berasumsi bahwa orang-orang dengan pendapatan yang sama dan jenis kelamin yang sama cenderung memiliki tingkat depresi yang sama. Untuk nilai yang hilang, kita akan melihat jenis kelamin orang tersebut, pendapatannya, mencari k tetangga terdekatnya dan mendapatkan tingkat depresi mereka. Kita kemudian dapat memperkirakan tingkat depresi orang yang kita inginkan.","title":"Missing Values using KNN"},{"location":"Tugas 3/#kalibrasi-parameter-knn","text":"","title":"Kalibrasi Parameter KNN"},{"location":"Tugas 3/#jumlah-tetangga-yang-harus-dicari","text":"Mengambil k rendah akan meningkatkan pengaruh kebisingan dan hasilnya akan kurang digeneralisasikan. Di sisi lain, mengambil k tinggi akan cenderung mengaburkan efek lokal yang persis apa yang kita cari. Juga disarankan untuk mengambil k yang aneh untuk kelas biner untuk menghindari ikatan.","title":"Jumlah tetangga yang harus dicari"},{"location":"Tugas 3/#metode-agregasi-untuk-digunakan","text":"Di sini kita memungkinkan untuk mean aritmatika, median dan mode untuk variabel numerik dan mode untuk yang kategorikal","title":"Metode agregasi untuk digunakan"},{"location":"Tugas 3/#normalisasi-data","text":"Ini adalah metode yang memungkinkan setiap atribut memberikan pengaruh yang sama dalam mengidentifikasi tetangga saat menghitung jenis jarak tertentu seperti yang Euclidean. Anda harus menormalkan data Anda ketika skala tidak memiliki arti dan / atau Anda memiliki skala tidak konsisten seperti sentimeter dan meter. Ini menyiratkan pengetahuan sebelumnya tentang data untuk mengetahui mana yang lebih penting. Algoritma secara otomatis menormalkan data ketika variabel numerik dan kategorikal disediakan.","title":"Normalisasi data"},{"location":"Tugas 3/#atribut-numerik-jarak","text":"Di antara berbagai metrik jarak yang tersedia, kami akan fokus pada yang utama, Euclidean dan Manhattan. Euclidean adalah ukuran jarak yang baik untuk digunakan jika variabel input bertipe sama (mis. Semua lebar dan tinggi yang diukur). Jarak Manhattan adalah ukuran yang baik untuk digunakan jika variabel input tidak dalam jenis yang sama (seperti usia, tinggi, dll ...).","title":"Atribut numerik jarak"},{"location":"Tugas 3/#atribut-kategorikal-jarak","text":"tanpa transformasi sebelumnya, jarak yang berlaku terkait dengan frekuensi dan kesamaan. Atribut kategorikal hampir sama dengan nominal karena dengan tipe ini akan dinormalisasikan menjadi numerik atau angka untuk bisa dirukur jaraknya # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 90 , np . nan , 95 ], 'Second Score' : [ 30 , 45 , 56 , np . nan ], 'Third Score' :[ np . nan , 40 , 80 , 98 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling a missing value with # previous ones df . fillna ( method = 'pad' ) First Score Second Score Third Score 0 100.0 30.0 NaN 1 90.0 45.0 40.0 2 90.0 56.0 80.0 3 95.0 56.0 98.0","title":"Atribut kategorikal jarak"},{"location":"Tugas 5 - Clustering/","text":"CLUSTERING \u00b6 Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi \u00b6 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means \u00b6 Metode K-Modes \u00b6 \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u00b6 \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u00b6 \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype \u00b6 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u00b6 \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u00b6 \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Tugas 5 - Clustering/#clustering","text":"Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"CLUSTERING"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-partisi","text":"1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Clustering dengan pendekatan partisi"},{"location":"Tugas 5 - Clustering/#algoritma-k-means","text":"\u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Algoritma K-Means"},{"location":"Tugas 5 - Clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Tugas 5 - Clustering/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Tugas 5 - Clustering/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Tugas 5 - Clustering/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Tugas 5 - Clustering/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/","text":"Regresi Linier Sederhana dan Berganda \u00b6 Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y). Regresi Linier Sederhana \u00b6 Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : Regresi Linier Berganda \u00b6 Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus dengan penghitungan manual \u00b6 Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi. Langkah 1 : Penentuan Tujuan \u00b6 Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat \u00b6 Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi Langkah 3 : Pengumpulan Data \u00b6 Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14 Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya \u00b6 Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya : Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana \u00b6 Langkah 6 : Buat Model Persamaan Regresi \u00b6 Y = a + bX Y = -24,38 + 1,45X Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat \u00b6 I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C Contoh kasus dengan penghitungan sklearn \u00b6 Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#regresi-linier-sederhana-dan-berganda","text":"Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y).","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#regresi-linier-sederhana","text":"Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut :","title":"Regresi Linier Sederhana"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#regresi-linier-berganda","text":"Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan)","title":"Regresi Linier Berganda"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#contoh-kasus-dengan-penghitungan-manual","text":"Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi.","title":"Contoh kasus dengan penghitungan manual"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-1-penentuan-tujuan","text":"Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali","title":"Langkah 1 : Penentuan Tujuan"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-2-identifikasikan-variabel-penyebab-dan-akibat","text":"Varibel Faktor Penyebab (X) : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi","title":"Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-3-pengumpulan-data","text":"Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14","title":"Langkah 3 : Pengumpulan Data"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-4-hitung-x2-y2-xy-dan-total-dari-masing-masingnya","text":"Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya :","title":"Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-5-hitung-a-dan-b-berdasarkan-rumus-regresi-linear-sederhana","text":"","title":"Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-6-buat-model-persamaan-regresi","text":"Y = a + bX Y = -24,38 + 1,45X","title":"Langkah 6 : Buat Model Persamaan Regresi"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#langkah-7-lakukan-prediksi-atau-peramalan-terhadap-variabel-faktor-penyebab-atau-variabel-akibat","text":"I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C","title":"Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat"},{"location":"Tugas 7 - Regresi Linier Sederhana dan Berganda/#contoh-kasus-dengan-penghitungan-sklearn","text":"Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh kasus dengan penghitungan sklearn"},{"location":"fuzzy clustering/","text":"fuzzy clustering \u00b6 pengertian fuzzy clustering: \u00b6 Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"fuzzy clustering"},{"location":"fuzzy clustering/#fuzzy-clustering","text":"","title":"fuzzy clustering"},{"location":"fuzzy clustering/#pengertian-fuzzy-clustering","text":"Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"pengertian fuzzy clustering:"},{"location":"jrak/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Tipe Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkanSebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Euclidean distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Chord distance \u00b6 Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ $$ dimana | x | {2} adalah L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} $$ Mahalanobis distance \u00b6 Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Tugas II Mengukur Jarak Data \u00b6 from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'data2.csv' , sep = \";\" ) k = df . iloc [ 10 : 17 ] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age of patient at time of operation Patient's year of operation Number of positive axillary nodes detected Unnamed: 3 10 34 60 1 1 11 34 61 10 1 12 34 67 7 1 13 34 60 0 1 14 35 64 13 1 15 35 63 0 1 16 36 60 1 1 numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0 Jarak numeric \u00b6 def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"menghitung jarak"},{"location":"jrak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"jrak/#mengukur-jarak-tipe-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkanSebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaitu v1,v2v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn,v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yixi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Tipe Numerik"},{"location":"jrak/#minkowski-distance","text":"Kelompk Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski distance. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$","title":"Minkowski Distance"},{"location":"jrak/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan distance"},{"location":"jrak/#euclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Euclidean distance"},{"location":"jrak/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,y dalam ruang dimensi n, rata-rata jarak didefinisikan dengan $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"jrak/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$","title":"Weighted euclidean distance"},{"location":"jrak/#chord-distance","text":"Chord distance adalah salah satu ukuran jarak modifikasi Euclidean distance untuk mengatasi kekurangan dari Euclidean distance. Ini dapat dipecahkan juga dengan menggunakan skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ $$ dimana | x | {2} adalah L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum_{ i = 1 }^{ n }x_{i}^{2}} $$","title":"Chord distance"},{"location":"jrak/#mahalanobis-distance","text":"Mahalanobis distance berdasarkan data berbeda dengan Euclidean dan Manhattan distances yang bebas antra data dengan data yang lain. Jarak Mahalanobis yang teratur dapat digunakan untuk mengekstraksi hyperellipsoidal clusters. Jarak Mahalanobis dapat mengurangi distorsi yang disebabkan oleh korelasi linier antara fitur dengan menerapkan transformasi pemutihan ke data atau dengan menggunakan kuadrat Jarak mahalanobis. Mahalanobis distance dinyatakan dengan $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$","title":"Mahalanobis distance"},{"location":"jrak/#tugas-ii-mengukur-jarak-data","text":"from scipy import stats import numpy as np import seaborn as sns import matplotlib.pyplot as plt import pandas as pd df = pd . read_csv ( 'data2.csv' , sep = \";\" ) k = df . iloc [ 10 : 17 ] k .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age of patient at time of operation Patient's year of operation Number of positive axillary nodes detected Unnamed: 3 10 34 60 1 1 11 34 61 10 1 12 34 67 7 1 13 34 60 0 1 14 35 64 13 1 15 35 63 0 1 16 36 60 1 1 numerical = [ 0 , 3 ] categorical = [ 1 , 2 , 6 , 7 ] binary = [ 4 , 5 , 8 ] ordinal = [ 1 , 2 ] from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] + [ 0 ] ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 0 0 0 0 v1-v3 0 0 0 0 0 v2-v3 0 0 0 0 0 v3-v4 0 0 0 0 0 v4-v5 0 0 0 0 0 v5-v6 0 0 0 0 0","title":"Tugas II Mengukur Jarak Data"},{"location":"jrak/#jarak-numeric","text":"def chordDist ( v1 , v2 , jnis ): jmlh = 0 normv1 = 0 normv2 = 0 for x in range ( len ( jnis )): normv1 = normv1 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) normv2 = normv2 + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) ** 2 ) jmlh = jmlh + ( int ( k . values . tolist ()[ v1 ][ jnis [ x ]]) * int ( k . values . tolist ()[ v2 ][ jnis [ x ]])) return (( 2 - ( 2 * jmlh / ( normv1 * normv2 ))) ** 0.5 ) from IPython.display import HTML , display import tabulate table = [ [ \"Data\" ] + [ \"Jarak\" ] + [ \"Numeric\" ] + [ \"Ordinal\" ] + [ \"Categorical\" ] + [ \"Binary\" ], [ \"v1-v2\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 1 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v1-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 0 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v2-v3\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 1 , 2 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v3-v4\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v4-v5\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 3 , 4 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], [ \"v5-v6\" ] + [ 0 ] + [ \"{:.2f}\" . format ( chordDist ( 2 , 3 , numerical ))] + [ 0 ] + [ 0 ] + [ 0 ], ] display ( HTML ( tabulate . tabulate ( table , tablefmt = 'html' ))) Data Jarak Numeric Ordinal Categorical Binary v1-v2 0 1.41 0 0 0 v1-v3 0 1.41 0 0 0 v2-v3 0 1.41 0 0 0 v3-v4 0 1.41 0 0 0 v4-v5 0 1.41 0 0 0 v5-v6 0 1.41 0 0 0 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Jarak numeric"},{"location":"tugas1/","text":"Mean, Modus dan Median \u00b6 Rumus Mean (Rata-rata) Data Kelompok \u00b6 Untuk dapat menentukan mean atau rata rata dari data kelompok maka kita perlu menjumlahkan semua data kemudian membaginya dengan banyaknya data tersebut.namun, karena penyajian data kelompok tersebut diberikan dalam bentuk yang berbeda, maka rumus untuk mencari nilai mean (rata rata) untuk data kelompok itu terlihat sedikit berbeda dengan cara mencari nilai mean (rata rata) pada data tunggal. Rumus mean data kelompok dinyatakan dengan persamaan seperti di bawah. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Rumus Median Data Kelompok \u00b6 Median ialah data tengah setelah diurutkan. Pada data tunggal, nilai median tersebut dapat dicari dengan mengurutkan datanya terlebih dahulu kemudian mencari data yang terletak tepat di tengahnya.cara ini Hampir sama dengan cara mencari median pada data tunggal, nilai median pada data kelompok juga merupakan nilai tengah dari suatu kumpulan data. Karena bentuk penyajian datanya disajikan dalam bentuk kelompok,maka datanya tidak dapat diurutkan seperti pada data tunggal. Dengan demikian, agar dapat mencari nilai median dari suatu data kelompok diperlukan sebuah rumus. Rumus median data kelompok ialah sebagai berikut. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ Rumus Modus Data Kelompok \u00b6 Modus ialah nilai data yang paling sering muncul atau data yang memiliki nilai frekuensi paling tinggi.untuk mencari nilai modus pada data tunggal sangat mudah,yaitu dengan Cara mencari nilai data dengan frekuensi paling banyak.namun untuk mencari mencari nilai modus pada data kelompok tidak lah semudah kita mencari nilai modus pada data tunggal. Hal ini dikarenakan bentuk penyajian data kelompok yang disajikan dalam sebuah rentang kelas. Sehingga, nilai modus data kelompok tidak mudah untuk langsung didapatkan dan untuk menemukan nilai modus dari data kelompok maka kita perlu menggunakan sebuah rumus. Rumus modus data kelompok dapat dilihat seperti persamaan di bawah ini. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Variansi dan Standar Deviasi \u00b6 Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N,x1,x2,...,xN, untuk atribut numerik X adalah $$ \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } $$ Skewness \u00b6 Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut $$ s k=\\frac{\\overline{X}-M o}{s} $$ dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka $$ s k \\approx \\frac{3(\\overline{X}-M e)}{s} $$ Tugas import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"tugas1/#mean-modus-dan-median","text":"","title":"Mean, Modus dan Median"},{"location":"tugas1/#rumus-mean-rata-rata-data-kelompok","text":"Untuk dapat menentukan mean atau rata rata dari data kelompok maka kita perlu menjumlahkan semua data kemudian membaginya dengan banyaknya data tersebut.namun, karena penyajian data kelompok tersebut diberikan dalam bentuk yang berbeda, maka rumus untuk mencari nilai mean (rata rata) untuk data kelompok itu terlihat sedikit berbeda dengan cara mencari nilai mean (rata rata) pada data tunggal. Rumus mean data kelompok dinyatakan dengan persamaan seperti di bawah. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$","title":"Rumus Mean (Rata-rata) Data Kelompok"},{"location":"tugas1/#rumus-median-data-kelompok","text":"Median ialah data tengah setelah diurutkan. Pada data tunggal, nilai median tersebut dapat dicari dengan mengurutkan datanya terlebih dahulu kemudian mencari data yang terletak tepat di tengahnya.cara ini Hampir sama dengan cara mencari median pada data tunggal, nilai median pada data kelompok juga merupakan nilai tengah dari suatu kumpulan data. Karena bentuk penyajian datanya disajikan dalam bentuk kelompok,maka datanya tidak dapat diurutkan seperti pada data tunggal. Dengan demikian, agar dapat mencari nilai median dari suatu data kelompok diperlukan sebuah rumus. Rumus median data kelompok ialah sebagai berikut. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$","title":"Rumus Median Data Kelompok"},{"location":"tugas1/#rumus-modus-data-kelompok","text":"Modus ialah nilai data yang paling sering muncul atau data yang memiliki nilai frekuensi paling tinggi.untuk mencari nilai modus pada data tunggal sangat mudah,yaitu dengan Cara mencari nilai data dengan frekuensi paling banyak.namun untuk mencari mencari nilai modus pada data kelompok tidak lah semudah kita mencari nilai modus pada data tunggal. Hal ini dikarenakan bentuk penyajian data kelompok yang disajikan dalam sebuah rentang kelas. Sehingga, nilai modus data kelompok tidak mudah untuk langsung didapatkan dan untuk menemukan nilai modus dari data kelompok maka kita perlu menggunakan sebuah rumus. Rumus modus data kelompok dapat dilihat seperti persamaan di bawah ini. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$","title":"Rumus Modus Data Kelompok"},{"location":"tugas1/#variansi-dan-standar-deviasi","text":"Variansi dan standar deviasi adalah ukuran penyebaran data. Nilai-nilai tersebut menunjukkan bagaimana penyebaran distribusi data. Standar Deviasi yang rendah berarti bahwa pengamatan data cenderung sangat dekat dengan rata-rata, sedangkan deviasi standar yang tinggi menunjukkan data tersebar di sejumlah nilai-nilai besar. Varian dari pengamatan N,x1,x2,...,xN, untuk atribut numerik X adalah $$ \\sigma ^ { 2 } = \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } ( x _ { i } - \\overline { x } ) ^ { 2 } = ( \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } ) - \\overline { x } ^ { 2 } $$","title":"Variansi dan Standar Deviasi"},{"location":"tugas1/#skewness","text":"Derajat distorsi dari kurva lonceng simetris atau distribusi normal. Ini mengukur kurangnya simetri dalam distribusi data Untuk menghitung derajat distorisi dapat menggunakan Koefisien Kemencengan Pearson yang diperoleh dengan menggunakan nilai selisih rata-rata dengan modus dibagi simpangan baku. Koefisien Kemencengan Pearson dirumuskan sebagai berikut $$ s k=\\frac{\\overline{X}-M o}{s} $$ dengan $$ \\overline{X}-M o \\approx 3(\\overline{X}-M e) $$ maka $$ s k \\approx \\frac{3(\\overline{X}-M e)}{s} $$ Tugas import pandas as pd from scipy import stats df = pd . read_csv ( 'data.csv' , sep = \";\" ) data = { \"stats\" :[ 'min' , 'max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quartile 1' , 'Quartile 2' , 'Quartile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . mean (), stats . mode ( df [ i ]) . mode [ 0 ]] kd = pd . DataFrame ( data ) kd . style . hide_index () MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Skewness"}]}